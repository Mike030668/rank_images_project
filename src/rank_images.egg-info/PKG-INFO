Metadata-Version: 2.4
Name: rank_images
Version: 0.1.0
Summary: Ранжирование изображений на основе текстовых промптов и качества с помощью SigLIP-2, Florence-2, CLIP-IQA и DINOv2.
Author-email: MikePuzitsky <puzitski.mikhail@gmail.com>
License: MIT
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.7.1
Requires-Dist: torchvision>=0.22.1
Requires-Dist: torchaudio>=2.7.1
Requires-Dist: accelerate
Requires-Dist: torchmetrics[multimodal]
Requires-Dist: transformers==4.53.1
Requires-Dist: sentence-transformers==4.1.0
Requires-Dist: fsspec>=2025.3.0
Requires-Dist: pyarrow==14.0.2
Requires-Dist: clip@ git+https://github.com/openai/CLIP.git
Requires-Dist: ftfy
Requires-Dist: regex
Requires-Dist: tqdm
Requires-Dist: einops
Requires-Dist: piq
Requires-Dist: numpy<2.0.0
Requires-Dist: pandas==2.2.2
Requires-Dist: sklearn-pandas==2.2.0
Requires-Dist: tqdm==4.67.1
Requires-Dist: Pillow
Requires-Dist: ipywidgets
Provides-Extra: dev
Requires-Dist: black; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Requires-Dist: isort; extra == "dev"
Provides-Extra: test
Requires-Dist: pytest; extra == "test"
Requires-Dist: pytest-cov; extra == "test"
Provides-Extra: notebook
Requires-Dist: jupyter; extra == "notebook"

# Rank Images Project

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Инструмент для ранжирования изображений на основе текстовых промптов и внутреннего качества, используя современные модели искусственного интеллекта: **SigLIP-2**, **Florence-2**, **CLIP-IQA**, **DINOv2**, **ImageReward**, **BLIP-2 (Image-Text Matching)**, **BLIP Caption + BERTScore** и **BLIP-2 Caption + BERTScore**.
Ранжирование происходит на основе комбинированного балла, рассчитанного из **восьми метрик**:

## Содержание

- [Описание](#описание)
  - [Формула итогового балла](#формула-итогового-балла)
- [Возможности](#возможности)
- [Установка](#установка)
- [Использование](#использование)
  - [Быстрый старт с демонстрацией](#быстрый-старт-с-демонстрацией)
  - [Ранжирование ваших изображений](#ранжирование-ваших-изображений)
  - [Настройка пайплайна с помощью JSON-конфигурации](#настройка-пайплайна-с-помощью-json-конфигурации)
  - [Аргументы командной строки](#аргументы-командной-строки)
- [Структура проекта](#структура-проекта)
- [Лицензия](#лицензия)

## Описание

Этот проект предоставляет скрипт командной строки (`rank-images`), который позволяет упорядочить коллекцию изображений. Ранжирование происходит на основе комбинированного балла, рассчитанного из **семи метрик**:

1.  **Схожесть изображения и текста (SigLIP-2):** Оценивает, насколько изображение соответствует позитивным текстовым запросам и отличается от негативных.
2.  **Поиск объектов по запросу (Florence-2):** Проверяет, насколько успешно Florence-2 может обнаружить объекты, описанные в позитивных промптах, на изображении. Для формирования итогового балла метрики также вычисляется средний скор для негативных промптов, и из результата для позитивных промптов вычитается результат для негативных.
3.  **Общее качество изображения (CLIP-IQA):** Дает оценку общего визуального качества изображения.
4.  **Внутренняя структура (DINOv2):** Косвенно оценивает "структурированность" или богатство деталей изображения.
5. **Human-Preference Score (ImageReward):** Обученная на 137k человеческих предпочтениях reward-модель (0–10), захватывающая композицию и стиль.
6.  **Соответствие изображения тексту (BLIP-2 ITM):** Оценивает вероятность соответствия изображения отдельным позитивным промптам, используя специализированную модель BLIP-2 для Image-Text Matching. Для формирования итогового балла метрики также вычисляется средний скор для негативных промптов, и из результата для позитивных промптов вычитается результат для негативных.
7.  **Соответствие описания промпту (BLIP Caption + BERTScore):** Генерирует описание изображения с помощью модели BLIP и оценивает семантическое сходство этого описания с исходным промптом с помощью метрики BERTScore.
8.  **Соответствие описания промпту (BLIP-2 Caption + BERTScore):** Генерирует описание изображения с помощью модели BLIP-2 и оценивает семантическое сходство этого описания с исходным промптом с помощью метрики BERTScore.

Каждая метрика имеет свой вес (alpha, beta, gamma, delta, epsilon, zeta, theta), который можно настроить. Модели оптимизированы для работы как на GPU (с экономией VRAM), так и на CPU.

### Формула итогового балла

Итоговый балл (`total`) для каждого изображения вычисляется как нормализованная взвешенная сумма значений **включённых** метрик. Каждая метрика сначала вычисляется с учетом позитивных и негативных промптов (если применимо), а затем нормализуется.

**Доступные метрики:**

1.  **`sig` (SigLIP-2):** Оценивает схожесть изображения и текста.
2.  **`flor` (Florence-2):** Проверяет, насколько успешно Florence-2 может обнаружить объекты, описанные в позитивных промптах.
3.  **`iqa` (CLIP-IQA):** Дает оценку общего визуального качества изображения.
4.  **`dino` (DINOv2):** Косвенно оценивает "структурированность" или богатство деталей изображения.
5.  **`blip2` (BLIP-2 ITM):** Оценивает вероятность соответствия изображения отдельным позитивным промптам.
6.  **`blip_cap` (BLIP Caption + BERTScore):** Оценивает семантическое сходство описания, сгенерированного BLIP, с исходным промптом.
7.  **`blip2_cap` (BLIP-2 Caption + BERTScore):** Оценивает семантическое сходство описания, сгенерированного BLIP-2, с исходным промптом.

**Общая логика вычисления:**

1.  **Вычисление исходных баллов метрик (`raw_score`)**:
    Для каждого изображения вычисляются **исходные** (ненормализованные) значения для **всех доступных** метрик. Используются только те метрики, которые указаны в списке `enabled_metrics` конфигурации пайплайна.
    *   **`sig`**:
        ```python
        raw_sig = get_siglip_score(image, positive_prompts) - get_siglip_score(image, negative_prompts)
        ```
    *   **`flor`**:
        ```python
        raw_flor = avg(get_florence_score(image, p) for p in positive_prompts) - avg(get_florence_score(image, n) for n in negative_prompts)
        ```
    *   **`iqa`**:
        ```python
        raw_iqa = get_iqa(image)
        ```
        *(Не использует промпты)*
    *   **`dino`**:
        ```python
        raw_dino = get_dino(image)
        ```
        *(Не использует промпты)*
    *   **`blip2`**:
        ```python
        raw_blip2 = get_blip2_match_score(image, positive_prompts) - get_blip2_match_score(image, negative_prompts)
        ```
    *   **`blip_cap`**:
        ```python
        raw_blip_cap = get_blip_caption_bertscore(image, prompt)
        ```
        *(Использует основной `prompt`, не разбивая на чанки)*
    *   **`blip2_cap`**:
        ```python
        raw_blip2_cap = get_blip2_caption_bertscore(image, prompt)
        ```
        *(Использует основной `prompt`, не разбивая на чанки)*

2.  **Нормализация по Z-оценке (`z_score`)**:
    После обработки всех изображений, для **каждой включённой** метрики (`m`) вычисляется Z-нормализованное значение на основе всех изображений в наборе:
    ```
    z_m = (raw_m_scores - mean(raw_m_scores)) / (std(raw_m_scores) + 1e-8)
    ```
    *(Добавление `1e-8` для численной стабильности.)*

3.  **Вычисление итогового нормализованного балла (`normalized_total`)**:
    Нормализованные баллы комбинируются с весами, **только если метрика включена**:
    ```
    normalized_total = sum(
        weight_map[m] * z_m for m in enabled_metrics_list
    )
    ```
    Где `weight_map` — это словарь, связывающий имя метрики с её весом:
    ```python
    weight_map = {
        "sig": alpha,      # Вес SigLIP-2
        "flor": beta,      # Вес Florence-2
        "iqa": gamma,      # Вес CLIP-IQA
        "dino": delta,     # Вес DINOv2
        "blip2": epsilon,  # Вес BLIP-2 ITM
        "blip_cap": zeta,  # Вес BLIP Caption + BERTScore
        "blip2_cap": theta,# Вес BLIP-2 Caption + BERTScore
    }
    ```

4.  **Финальный итоговый балл (`total`)**:
    Чтобы получить значение в привычном диапазоне и учесть сумму используемых весов, итоговый балл нормализуется:
    ```
    total = normalized_total / sum(weight_map[m] for m in enabled_metrics_list)
    ```
    Если сумма используемых весов равна 0, `total` устанавливается в 0.

**Важно:** Весовые коэффициенты (`alpha`, `beta`, `gamma`, `delta`, `epsilon`, `zeta`, `theta`) по умолчанию подобраны для демонстрационных данных. Для получения наилучших результатов на ваших собственных наборах изображений **необходимо вручную подбирать** эти коэффициенты. Это позволяет точно настроить вклад каждой метрики в итоговое ранжирование, особенно при работе с разнородными примерами.


Каждая метрика имеет свой вес (alpha, beta, gamma, delta, epsilon), который можно настроить. Модели оптимизированы для работы как на GPU (с экономией VRAM), так и на CPU.

## Возможности

-   **Экономия VRAM:** Модели SigLIP-2, DINOv2, BLIP-2 ITM, BLIP Caption и BLIP-2 Caption временно загружаются на GPU только во время вычислений. Florence-2 использует автоматическое распределение (`device_map="auto"`).
-   **Поддержка GPU и CPU:** Автоматическое определение наличия CUDA.
-   **Гибкие промпты:** Поддержка JSON-файлов с позитивными/негативными промптами или простых текстовых строк.
-   **Разбиение длинных текстов:** Автоматическое разбиение длинных промптов для SigLIP.
-   **Расширяемость:** Код структурирован для легкого добавления новых метрик.
-   **Гибкая настройка пайплайна:** Используйте JSON-файлы для определения, какие метрики использовать и с какими весами. Это позволяет легко создавать профили для разных задач ранжирования.
-   **Эффективность ресурсов:** Оптимизированная загрузка и использование моделей позволяют запускать проект на ограниченных ресурсах (например, локально с ~16GB RAM).

## Установка

Для установки проекта рекомендуется использовать `pip` или `uv pip`.

**Важно:** Проект требует **PyTorch >= 2.7.1**. Для его установки необходимо использовать **nightly** сборки с индекса PyTorch.

### Вариант 1: Используя `pip`

```bash
# 1. Клонируйте репозиторий
git clone https://github.com/Mike030668/rank_images_project.git
cd rank_images_project

# 2. (Рекомендуется) Создайте виртуальное окружение
python -m venv venv
# Активируйте его:
#   Linux/macOS: source venv/bin/activate
#   Windows:     venv\Scripts\activate

# 3. # 3. Установите проект и зависимости (editable mode)
   #    Используем --pre и дополнительный индекс для установки nightly PyTorch
#  (ImageReward требует git-репозиторий CLIP → укажем его явно)
pip install --pre \
  --extra-index-url https://download.pytorch.org/whl/nightly/cu121 \
  -e . \
  "clip @ git+https://github.com/openai/CLIP.git"

# 4. (Опционально) Установите дополнительные зависимости
# Для Jupyter-ноутбуков:
# pip install -e .[notebook]
# Для метрик, использующих BERTScore (BLIP Caption, BLIP-2 Caption):
pip install -e .[caption]
```

### Вариант 2: Используя `uv pip` (если установлен `uv`)

`uv` значительно ускоряет установку зависимостей.

```bash
# 1. Клонируйте репозиторий
git clone https://github.com/Mike030668/rank_images_project.git
cd rank_images_project

# 2. (Рекомендуется) Создайте виртуальное окружение с помощью uv
uv venv

# 3. Активируйте его:
#   Linux/macOS: source .venv/bin/activate
#   Windows:     .venv\Scripts\activate

# 4. Установите проект и все основные зависимости
uv pip install --prerelease=allow \
  --extra-index-url https://download.pytorch.org/whl/nightly/cu121 \
  --index-strategy unsafe-best-match \
  -e . \
  "clip @ git+https://github.com/openai/CLIP.git"

# 5. (Опционально) Установите расширения
#   • Jupyter-ноутбуки
#       uv pip install -e .[notebook]
#   • Метрики на основе BERTScore (BLIP Caption, BLIP-2 Caption)
        uv pip install -e .[caption]
```

## Использование

После установки вы можете использовать команду `rank-images` из любого места, если ваше виртуальное окружение активно.

### Быстрый старт с демонстрацией

Проект включает демонстрационные изображения и промпты. Чтобы быстро проверить работу:

```bash
rank-images --demo
```

Это выполнит ранжирование изображений из папки `data/demo_images/` с использованием `data/demo_images/prompts.json`. Результат будет сохранен в `data/demo_images/ranking.csv`.

### Ранжирование ваших изображений

```bash
rank-images /путь/к/вашей/папке/с/изображениями --prompts /путь/к/prompts.json
```

**Пример:**

1.  Положите ваши изображения (`.jpg`, `.png`) в папку, например, `my_images/`.
2.  Создайте файл `my_prompts.json`:

    ```json
    {
      "prompt": "clear photo of a modern skyscraper, blue sky, high resolution",
      "prompt2": "sharp focus, detailed architecture",
      "negative": "blurry, low resolution, watermark, cartoon",
      "negative2": "night time, dark"
    }
    ```
3.  Запустите ранжирование:

    ```bash
    rank-images my_images --prompts my_prompts.json
    ```

4.  Результат будет записан в `my_images/ranking.csv`. Изображения будут отсортированы по убыванию итогового балла (`total`).

### Настройка пайплайна с помощью JSON-конфигурации

Вы можете определить, какие метрики использовать и с какими весами по умолчанию, с помощью JSON-файла конфигурации.

```bash
rank-images /путь/к/вашей/папке/с/изображениями \
  --prompts /путь/к/prompts.json \
  --pipeline-config configs/caption_focus_pipeline.json
```

**Примеры конфигурационных файлов:**

Файлы конфигурации находятся в папке `configs/`.

*   `configs/default_pipeline.json`: Использует все метрики с весами по умолчанию.
*   `configs/caption_focus_pipeline.json`: Фокусируется на метриках, связанных с генерацией описаний (captioning).
*   `configs/quality_and_alignment_pipeline.json`: Делает акцент на качестве изображения и соответствии запросу.

**Создание собственного конфига:**

Создайте файл, например, `my_custom_pipeline.json`:

```json
{
  "pipeline": {
    "enabled_metrics": ["sig", "flor", "iqa"],
    "default_weights": {
      "alpha": 0.5,
      "beta": 0.3,
      "gamma": 0.2,
      "delta": 0.0,
      "epsilon": 0.0,
      "zeta": 0.0,
      "theta": 0.0
    }
  },
  "processing": {
    "chunk_size": null
  }
}
```

И используйте его:

```bash
rank-images my_images --prompts my_prompts.json --pipeline-config my_custom_pipeline.json
```

### Аргументы командной строки

```bash
rank-images --help
```

```
usage: rank-images [-h] [--prompts PROMPTS] [--alpha ALPHA] [--beta BETA]
                   [--gamma GAMMA] [--delta DELTA] [--epsilon EPSILON]
                   [--zeta ZETA] [--theta THETA] [--chunk CHUNK] [--demo]
                   [--pipeline-config PIPELINE_CONFIG]
                   img_dir

Ранжирует изображения в заданной директории на основе текстовых промптов и
внутреннего качества, используя SigLIP-2, Florence-2, CLIP-IQA, DINOv2,
BLIP-2 (ITM) и BLIP-2/Florence-2 Caption + BERTScore.

positional arguments:
  img_dir               Путь к директории с изображениями для ранжирования.

options:
  -h, --help            show this help message and exit
  --prompts PROMPTS     Источник текстовых промптов. Может быть:
                        - Путь к .json файлу с ключами 'prompt', 'prompt2',
                          'negative', 'negative2'.
                        - Произвольная текстовая строка (будет использована
                          как 'prompt').
                        Если не указан, ранжирование будет основано только на
                        именах файлов.
  --alpha ALPHA         Вес метрики SigLIP (схожесть изображения и текста).
                        По умолчанию 0.6.
  --beta BETA           Вес метрики Florence-2 (поиск объектов по запросу).
                        По умолчанию 0.4.
  --gamma GAMMA         Вес метрики CLIP-IQA (общее качество изображения).
                        По умолчанию 0.2.
  --delta DELTA         Вес метрики DINOv2 (внутренние признаки изображения).
                        По умолчанию 0.1.
  --epsilon EPSILON     Вес метрики BLIP-2 (Image-Text Matching).
                        По умолчанию 0.3.
  --zeta ZETA           Вес метрики BLIP Caption + BERTScore к prompt.
                        По умолчанию 0.25.
  --theta THETA         Вес метрики BLIP-2 Caption + BERTScore к prompt.
                        По умолчанию 0.2.
  --chunk CHUNK         Максимальное количество токенов в одном фрагменте
                        текста для SigLIP.
                        Используется для разбиения длинных текстов. По
                        умолчанию используется значение из config.py.
  --demo                Запустить демонстрацию на встроенных примерах из
                        data/demo_images/.
  --pipeline-config PIPELINE_CONFIG
                        Путь к JSON-файлу конфигурации пайплайна. Определяет,
                        какие метрики включены и их веса по умолчанию. Если не
                        указан, используется стандартная конфигурация.
```

## Структура проекта

Структура проекта следует рекомендациям Cookiecutter Data Science для организации кода и данных.

```
rank_images_project/
├── README.md                 # Этот файл
├── pyproject.toml            # Конфигурация проекта и зависимостей
├── .gitignore                # Файлы и папки, игнорируемые Git
├── configs/                  # JSON-файлы конфигурации пайплайна
│   ├── default_pipeline.json # Конфигурация по умолчанию
│   ├── caption_focus_pipeline.json # Профиль: фокус на captioning
│   ├── quality_and_alignment_pipeline.json # Профиль: качество и соответствие
│   └── ...                   # Другие пользовательские профили
├── data/
│   ├── raw/                  # (Опционально) Ваши собственные изображения для обработки
│   └── demo_images/          # Демонстрационные изображения и промпты
│       ├── image1.jpg        # Пример изображения 1
│       ├── image2.png        # Пример изображения 2
│       └── prompts.json     # Промпты для демонстрационных изображений
├── notebooks/                # Jupyter ноутбуки для исследований
├── src/
│   └── rank_images/          # Основной пакет Python
│       ├── __init__.py       # Делает каталог пакетом
│       ├── config.py         # Глобальные константы
│       ├── device_utils.py   # Функции управления устройствами (_to_gpu, _release)
│       ├── models.py         # Загрузка и хранение моделей
│       ├── data_processing.py# Вспомогательные функции обработки данных
│       ├── metrics.py        # Функции вычисления метрик
│       ├── utils.py          # Общие утилиты (нормализация и т.д.)
│       ├── example_metric.py # Пример шаблона для добавления новой метрики
│       ├── pipeline_config.py # Управление конфигурацией пайплайна (JSON)
│       ├── ranking.py         # Основная логика ранжирования
│       └── cli.py            # Интерфейс командной строки
└── outputs/                  # (Игнорируется) Для будущих результатов/промежуточных данных
```

## Лицензия

Этот проект лицензирован по лицензии MIT - см. файл [LICENSE](LICENSE) для подробностей.
