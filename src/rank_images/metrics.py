# src/rank_images/metrics.py
"""
–§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å —Å–æ–¥–µ—Ä–∂–∏—Ç —Ñ—É–Ω–∫—Ü–∏–∏, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö —Ä–µ–∞–ª–∏–∑—É–µ—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é –º–µ—Ç—Ä–∏–∫—É
–¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π. –≠—Ç–æ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –º–æ–¥—É–ª—å –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏:
–¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é –∑–¥–µ—Å—å –∏ –∑–∞—Ç–µ–º
–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –µ—ë –≤ –ª–æ–≥–∏–∫—É —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –≤ `ranking.py`.

–í—Å–µ —Ñ—É–Ω–∫—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–Ω–∞–±–∂–µ–Ω—ã –¥–µ–∫–æ—Ä–∞—Ç–æ—Ä–æ–º `@torch.inference_mode()`
–¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤–æ –≤—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞.

–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:
    –§—É–Ω–∫—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ SigLIP-2 –∏ DINOv2, –¥–æ–ª–∂–Ω—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
    `_to_gpu` –∏ `_release` –∏–∑ `device_utils` –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è
    –º–æ–¥–µ–ª–∏ –Ω–∞ GPU –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è –ø–∞–º—è—Ç–∏.
"""
import logging
from typing import List, Optional
import numpy as np
import torch
import torchvision
from PIL import Image

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logger = logging.getLogger(__name__)
# --- –ò–ú–ü–û–†–¢ BERTSCORE ---
try:
    from bert_score import score as bert_score_func
    BERT_SCORE_AVAILABLE = True
except ImportError:
    bert_score_func = None
    BERT_SCORE_AVAILABLE = False
    logger.warning(
        "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ `bert_score` –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ú–µ—Ç—Ä–∏–∫–∞ BLIP Caption + BERTScore –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. "
        "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ—ë —Å –ø–æ–º–æ—â—å—é `pip install bert_score`."
    )
# -----------------------

# –ò–º–ø–æ—Ä—Ç —É—Ç–∏–ª–∏—Ç –∏ –º–æ–¥–µ–ª–µ–π
from . import models
from .device_utils import _to_gpu, _release
from .config import MAX_SIG_TOK, DTYPE, DEVICE_CPU


# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ ---
def _z(a: np.ndarray) -> np.ndarray:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –º–∞—Å—Å–∏–≤ –ø–æ Z-–æ—Ü–µ–Ω–∫–µ (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è).

    –í—ã—á–∏—Ç–∞–µ—Ç —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–∞—Å—Å–∏–≤–∞ –∏ –¥–µ–ª–∏—Ç –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ.
    –ï—Å–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ —Ä–∞–≤–Ω–æ 0, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞—Å—Å–∏–≤ –Ω—É–ª–µ–π —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã.

    Args:
        a (np.ndarray): –í—Ö–æ–¥–Ω–æ–π –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π –º–∞—Å—Å–∏–≤ NumPy.

    Returns:
        np.ndarray: –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—Å–∏–≤ —Ç–æ–π –∂–µ —Ñ–æ—Ä–º—ã.
    """
    std_dev = a.std()
    if std_dev > 0:
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: (x - mean) / std
        return (a - a.mean()) / std_dev
    else:
        # –ï—Å–ª–∏ std == 0, –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –º–∞—Å—Å–∏–≤ –Ω—É–ª–µ–π
        return np.zeros_like(a)


# --- –ú–µ—Ç—Ä–∏–∫–∏ ---
# –ö–∞–∂–¥–∞—è –º–µ—Ç—Ä–∏–∫–∞ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ PIL.Image.Image –∏, –≤–æ–∑–º–æ–∂–Ω–æ,
# –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã).
# –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ (float) –∫–∞–∫ –æ—Ü–µ–Ω–∫—É.

#print(f"[DEBUG_IMPORT] –í metrics.py –ø–æ—Å–ª–µ –∏–º–ø–æ—Ä—Ç–æ–≤: sig_proc is None = {models.sig_proc is None}")

@torch.inference_mode()
def get_siglip_score(img: Image.Image, txts: List[str]) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω—é—é –∫–æ—Å–∏–Ω—É—Å–Ω—É—é —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∏ —Å–ø–∏—Å–∫–æ–º —Ç–µ–∫—Å—Ç–æ–≤
    —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ SigLIP-2.

    –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤ —Å–ø–∏—Å–∫–µ `txts` –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å—Ö–æ–∂–µ—Å—Ç—å —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —ç—Ç–∏—Ö —Å—Ö–æ–∂–µ—Å—Ç–µ–π.

    Args:
        img (PIL.Image.Image): –í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
        txts (List[str]): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.

    Returns:
        float: –°—Ä–µ–¥–Ω—è—è –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å. –ï—Å–ª–∏ `txts` –ø—É—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0.0.
    """
    #print(f"[DEBUG_CALL] –í get_siglip_score: sig_proc is None = {sig_proc is None}")

    if models.sig_proc is None:
        raise RuntimeError(
            "models.sig_proc —Ä–∞–≤–µ–Ω None –≤ get_siglip_score! "
            "–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ load_models() –±—ã–ª–∞ –≤—ã–∑–≤–∞–Ω–∞ –¥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è."
        )

    if not txts:
        logger.debug("–°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è SigLIP –ø—É—Å—Ç. –í–æ–∑–≤—Ä–∞—â–∞—é 0.0.")
        return 0.0

    logger.debug(f"–í—ã—á–∏—Å–ª—è—é SigLIP-2 —Å–∫–æ—Ä –¥–ª—è {len(txts)} —Ç–µ–∫—Å—Ç–æ–≤.")

    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
    #    —Å —Ä–∞–∑–±–∏–µ–Ω–∏–µ–º —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –ø–æ MAX_SIG_TOK —Ç–æ–∫–µ–Ω–æ–≤.
    #    `sig_proc` —É–∂–µ –∑–Ω–∞–µ—Ç –æ MAX_SIG_TOK, –Ω–æ –º—ã —è–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è.
    feats = models.sig_proc( # <-- –û–±—Ä–∞—â–µ–Ω–∏–µ —á–µ—Ä–µ–∑ models.
        images=img,
        text=txts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=MAX_SIG_TOK,
    )

    # 2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Ö–æ–¥–Ω—ã—Ö —Ç–µ–Ω–∑–æ—Ä–æ–≤ (–∫—Ä–æ–º–µ input_ids)
    feats = {
        k: v.to(dtype=DTYPE) if k != "input_ids" else v for k, v in feats.items()
    }

    # 3. –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω) –∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    model_gpu = _to_gpu(models.sig_model) # <-- –û–±—Ä–∞—â–µ–Ω–∏–µ —á–µ—Ä–µ–∑ models.
    feats = {k: v.to(model_gpu.device) for k, v in feats.items()}

    # 4. –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
    try:
        out = model_gpu(**feats)
    finally:
        # 5. –í—Å–µ–≥–¥–∞ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        _release(model_gpu)

    # 6. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    im_embeds = torch.nn.functional.normalize(out.image_embeds, dim=-1)
    txt_embeds = torch.nn.functional.normalize(out.text_embeds, dim=-1)

    # 7. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –∏ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏–µ
    #    (im_embeds @ txt_embeds.T) –¥–∞–µ—Ç –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–∂–µ—Å—Ç–µ–π [1 x N_txts]
    similarities = (im_embeds @ txt_embeds.T).squeeze(0) # [N_txts]
    mean_similarity = similarities.mean().item()

    logger.debug(f"–°—Ä–µ–¥–Ω—è—è SigLIP-2 —Å—Ö–æ–∂–µ—Å—Ç—å: {mean_similarity:.4f}")
    return mean_similarity


@torch.inference_mode()
def get_florence_score(img: Image.Image, phrase: str) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç "–∫–∞—á–µ—Å—Ç–≤–æ grounding" —Ñ—Ä–∞–∑—ã –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å –ø–æ–º–æ—â—å—é Florence-2.

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∑–∞–¥–∞—á—É `<CAPTION_TO_PHRASE_GROUNDING>` –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤,
    –æ–ø–∏—Å–∞–Ω–Ω—ã—Ö –≤ `phrase`. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤
    –∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤ –≤ —Ñ—Ä–∞–∑–µ.

    –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:
        –ú–æ–¥–µ–ª—å Florence-2 –ù–ï –ø–µ—Ä–µ–º–µ—â–∞–µ—Ç—Å—è `_to_gpu`, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ —É–∂–µ
        –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ —Ä–∞–∑–º–µ—â–µ–Ω–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ (`device_map="auto"`).

    Args:
        img (PIL.Image.Image): –í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
        phrase (str): –¢–µ–∫—Å—Ç–æ–≤–∞—è —Ñ—Ä–∞–∑–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–±—ä–µ–∫—Ç–æ–≤.

    Returns:
        float: –û—Ç–Ω–æ—à–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤ –≤ —Ñ—Ä–∞–∑–µ.
               –ï—Å–ª–∏ —Ñ—Ä–∞–∑–∞ –ø—É—Å—Ç–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0.0.
    """
    phrase = phrase.strip()
    if not phrase:
        logger.debug("–§—Ä–∞–∑–∞ –¥–ª—è Florence –ø—É—Å—Ç–∞. –í–æ–∑–≤—Ä–∞—â–∞—é 0.0.")
        return 0.0

    logger.debug(f"–í—ã—á–∏—Å–ª—è—é Florence-2 —Å–∫–æ—Ä –¥–ª—è —Ñ—Ä–∞–∑—ã: '{phrase[:50]}...'")

    # 1. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏
    task = "<CAPTION_TO_PHRASE_GROUNDING>"

    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    inputs = models.flor_proc(text=task + phrase, images=img, return_tensors="pt")

    # 3. –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, –≥–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –º–æ–¥–µ–ª—å
    #    –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –∏ —Ç–∏–ø –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–≤–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –º–æ–¥–µ–ª–∏
    first_param = next(models.flor_model.parameters())
    target_device = first_param.device
    model_dtype = first_param.dtype

    #    –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Ç–µ–Ω–∑–æ—Ä—ã –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    #    pixel_values —á–∞—Å—Ç–æ –∏–º–µ—é—Ç –¥—Ä—É–≥–æ–π —Ç–∏–ø (float), –ø–æ—ç—Ç–æ–º—É –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
    inputs_moved = {}
    for k, v in inputs.items():
        if k == "pixel_values":
            inputs_moved[k] = v.to(target_device, dtype=model_dtype)
        else:
            inputs_moved[k] = v.to(target_device)

    # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (–∏–Ω—Ñ–µ—Ä–µ–Ω—Å)
    generated_ids = models.flor_model.generate(
        input_ids=inputs_moved["input_ids"],
        pixel_values=inputs_moved["pixel_values"],
        max_new_tokens=512,
        do_sample=False,
        num_beams=3,
    )

    # 5. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    generated_text = models.flor_proc.batch_decode(generated_ids, skip_special_tokens=False)[0]

    # 6. –ü–æ—Å—Ç-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    parsed_output = models.flor_proc.post_process_generation(
        generated_text, task=task, image_size=(img.width, img.height)
    )

    # 7. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ (labels)
    labels_found = parsed_output.get(task, {}).get("labels", [])
    num_labels_found = len(labels_found)

    # 8. –ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–≤ –≤ —Ñ—Ä–∞–∑–µ
    #    –ó–∞–º–µ–Ω—è–µ–º –∑–∞–ø—è—Ç—ã–µ –Ω–∞ –ø—Ä–æ–±–µ–ª—ã –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞, –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤—ã–≤–∞—è –ø—É—Å—Ç—ã–µ
    words_in_phrase = [w for w in phrase.replace(",", " ").split() if w]
    num_words = len(words_in_phrase)

    # 9. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –±–∞–ª–ª–∞
    score = num_labels_found / max(num_words, 1) # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0

    logger.debug(
        f"Florence-2: –Ω–∞–π–¥–µ–Ω–æ {num_labels_found} –æ–±—ä–µ–∫—Ç–æ–≤ –∏–∑ {num_words} —Å–ª–æ–≤. "
        f"–°–∫–æ—Ä: {score:.4f}"
    )
    return score


@torch.inference_mode()
def get_iqa(img: Image.Image) -> float:
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –æ–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ CLIP-IQA.

    –ú–æ–¥–µ–ª—å –≤—Å–µ–≥–¥–∞ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ CPU.

    Args:
        img (PIL.Image.Image): –í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.

    Returns:
        float: –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
    """
    logger.debug("–í—ã—á–∏—Å–ª—è—é CLIP-IQA —Å–∫–æ—Ä.")

    # 1. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ PIL Image –≤ —Ç–µ–Ω–∑–æ—Ä PyTorch –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ batch dimension
    #    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ float32, —Ç–∞–∫ –∫–∞–∫ iqa_metric –æ–∂–∏–¥–∞–µ—Ç —ç—Ç–æ—Ç —Ç–∏–ø –Ω–∞ CPU
    img_tensor = torchvision.transforms.functional.to_tensor(img).unsqueeze(0)
    img_tensor = img_tensor.to(DEVICE_CPU, dtype=torch.float32)

    # 2. –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å (–Ω–∞ CPU)
    quality_score = models.iqa_metric(img_tensor).item()

    logger.debug(f"CLIP-IQA —Å–∫–æ—Ä: {quality_score:.4f}")
    return quality_score


@torch.inference_mode()
def get_dino(img: Image.Image) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç L2-–Ω–æ—Ä–º—É –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ CLS-—Ç–æ–∫–µ–Ω–∞ –∏–∑ –º–æ–¥–µ–ª–∏ DINOv2.

    –≠—Ç–æ –∫–æ—Å–≤–µ–Ω–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞ "–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞" –∏–ª–∏ "—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏"
    –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

    Args:
        img (PIL.Image.Image): –í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.

    Returns:
        float: L2-–Ω–æ—Ä–º–∞ –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ CLS-—Ç–æ–∫–µ–Ω–∞.
    """
    logger.debug("–í—ã—á–∏—Å–ª—è—é DINOv2 —Å–∫–æ—Ä.")

    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
    feats = models.dino_proc(images=img, return_tensors="pt")

    # 2. –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω) –∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    model_gpu = _to_gpu(models.dino_model)
    feats = {k: v.to(model_gpu.device, dtype=DTYPE) for k, v in feats.items()}

    # 3. –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å
    try:
        output = model_gpu(**feats)
        # 4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ CLS-—Ç–æ–∫–µ–Ω–∞ (–ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
        cls_token_features = output.last_hidden_state[:, 0, :] # [1, D]
    finally:
        # 5. –í—Å–µ–≥–¥–∞ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        _release(model_gpu)

    # 6. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ L2-–Ω–æ—Ä–º—ã –≤–µ–∫—Ç–æ—Ä–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    l2_norm = torch.linalg.vector_norm(cls_token_features, ord=2, dim=-1).item()

    logger.debug(f"DINOv2 L2-–Ω–æ—Ä–º–∞ CLS-—Ç–æ–∫–µ–Ω–∞: {l2_norm:.4f}")
    return l2_norm


# --- –ù–û–í–ê–Ø –ú–ï–¢–†–ò–ö–ê: BLIP-2 Matching Score ---# --- –ù–û–í–ê–Ø –ú–ï–¢–†–ò–ö–ê: BLIP-2 Matching Score ---
@torch.inference_mode()
def get_blip2_match_score(img: Image.Image, prompts: List[str]) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω—é—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è (Image-Text Matching) –º–µ–∂–¥—É
    –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º –∏ —Å–ø–∏—Å–∫–æ–º —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ BLIP-2 ITM.

    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥—ã–π –ø—Ä–æ–º–ø—Ç –æ—Ç–¥–µ–ª—å–Ω–æ.

    Args:
        img (PIL.Image.Image): –í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
        prompts (List[str]): –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ.

    Returns:
        float: –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è. –ï—Å–ª–∏ —Å–ø–∏—Å–æ–∫ `prompts` –ø—É—Å—Ç
               –∏–ª–∏ –º–æ–¥–µ–ª—å BLIP-2 –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0.0.
    """
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å
    if models.blip2_processor is None or models.blip2_model is None:
        logger.warning(
            "–ú–æ–¥–µ–ª—å –∏–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä BLIP-2 (ITM) –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. "
            "–í–æ–∑–≤—Ä–∞—â–∞—é 0.0 –¥–ª—è BLIP-2 —Å–∫–æ—Ä."
        )
        return 0.0

    if not prompts:
        logger.debug("–°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è BLIP-2 –ø—É—Å—Ç. –í–æ–∑–≤—Ä–∞—â–∞—é 0.0.")
        return 0.0

    logger.debug(f"–í—ã—á–∏—Å–ª—è—é BLIP-2 ITM —Å–∫–æ—Ä –¥–ª—è {len(prompts)} –ø—Ä–æ–º–ø—Ç–æ–≤.")

    individual_scores = []
    model_gpu = None

    try:
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω) –æ–¥–∏–Ω —Ä–∞–∑
        model_gpu = _to_gpu(models.blip2_model)

        for prompt in prompts:
            try:
                # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –û–î–ù–û–ì–û —Ç–µ–∫—Å—Ç–∞
                inputs = models.blip2_processor(
                    images=img,
                    text=prompt, # –û–¥–∏–Ω —Ç–µ–∫—Å—Ç
                    return_tensors="pt",
                    padding=True,
                    truncation=True
                )

                # 2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö
                if "pixel_values" in inputs:
                    inputs["pixel_values"] = inputs["pixel_values"].to(dtype=DTYPE)

                # 3. –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
                inputs_moved = {k: v.to(model_gpu.device) for k, v in inputs.items()}

                # 4. –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å ITM —Å —Ñ–ª–∞–≥–æ–º
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º logits_per_image, –∫–∞–∫ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º –ø—Ä–∏–º–µ—Ä–µ
                outputs = model_gpu(
                    **inputs_moved,
                    use_image_text_matching_head=True
                )

                # 5. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ª–æ–≥–∏—Ç–æ–≤
                # –°–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∏–º–µ—Ä—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º logits_per_image
                logits_per_image = outputs.logits_per_image # –û–∂–∏–¥–∞–µ–º–∞—è —Ñ–æ—Ä–º–∞: [1, N_classes]
                logger.debug(f"  –ü—Ä–æ–º–ø—Ç '{prompt[:20]}...': logits_per_image.shape = {logits_per_image.shape}")

                if logits_per_image.dim() != 2 or logits_per_image.shape[0] != 1:
                    logger.warning(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞ logits_per_image –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ '{prompt}': {logits_per_image.shape}")
                    continue

                num_classes = logits_per_image.shape[1]
                if num_classes < 1:
                    logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –≤ logits_per_image –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ '{prompt}': {logits_per_image.shape}")
                    continue

                # –ü—Ä–∏–º–µ–Ω—è–µ–º softmax –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
                # –°–æ–≥–ª–∞—Å–Ω–æ –ø—Ä–∏–º–µ—Ä—É, softmax –ø–æ dim=1
                probs = torch.nn.functional.softmax(logits_per_image, dim=1)

                # 6. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ "yes"
                # probs.shape [1, N_classes]
                if num_classes >= 2:
                    # probs[0, 1] - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å "yes" –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ (–∏ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ) –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                    match_prob = probs[0, 1].item()
                else:
                    # –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –∫–ª–∞—Å—Å, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —ç—Ç–æ –∏ –µ—Å—Ç—å "yes"
                    # –∏–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –Ω–∞–ø—Ä—è–º—É—é
                    match_prob = probs[0, -1].item()

                individual_scores.append(match_prob)
                logger.debug(f"  –ü—Ä–æ–º–ø—Ç '{prompt[:20]}...': –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è = {match_prob:.4f}")

            except Exception as prompt_e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –ø—Ä–æ–º–ø—Ç–∞ '{prompt}': {prompt_e}")
                continue

        # 7. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
        if individual_scores:
            average_match_prob = sum(individual_scores) / len(individual_scores)
            logger.debug(f"–°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è BLIP-2 (ITM) –ø–æ {len(individual_scores)} –ø—Ä–æ–º–ø—Ç–∞–º: {average_match_prob:.4f}")
            return average_match_prob
        else:
            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–∏ –æ–¥–∏–Ω —Å–∫–æ—Ä –¥–ª—è BLIP-2 ITM.")
            return 0.0

    except Exception as e:
        logger.error(f"–§–∞—Ç–∞–ª—å–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ BLIP-2 —Å–∫–æ—Ä: {e}", exc_info=True)
        return 0.0
    finally:
        # 8. –í—Å–µ–≥–¥–∞ –æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        if model_gpu is not None:
            _release(model_gpu)
# --- –ö–æ–Ω–µ—Ü –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏ ---

# --- –ù–û–í–ê–Ø –ú–ï–¢–†–ò–ö–ê: BLIP Caption + BERTScore ---
@torch.inference_mode()
def get_blip_caption_bertscore(img: Image.Image, prompt: str) -> float:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç BERTScore –º–µ–∂–¥—É –æ–ø–∏—Å–∞–Ω–∏–µ–º (caption), —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –º–æ–¥–µ–ª—å—é BLIP,
    –∏ –∏—Å—Ö–æ–¥–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–≤—ã–º –ø—Ä–æ–º–ø—Ç–æ–º.

    Args:
        img (PIL.Image.Image): –í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
        prompt (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç.

    Returns:
        float: –°—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ F1-–º–µ—Ä—ã BERTScore. –ï—Å–ª–∏ `prompt` –ø—É—Å—Ç,
               –º–æ–¥–µ–ª—å BLIP Caption –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, BERTScore –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
               –∏–ª–∏ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0.0.
    """

    
    # --- –û–¢–õ–ê–î–ö–ê ---
    logger.debug(f"[BLIP_CAP_DEBUG] –í—ã–∑–æ–≤ get_blip_caption_bertscore —Å img={img}, prompt='{prompt[:50]}...'")
    # --------------
    # –ü—Ä–æ–≤–µ—Ä–∫–∞, –∑–∞–≥—Ä—É–∂–µ–Ω—ã –ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
    if not BERT_SCORE_AVAILABLE:
        logger.debug("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ `bert_score` –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –í–æ–∑–≤—Ä–∞—â–∞—é 0.0.")
        return 0.0

    if models.blip_cap_processor is None or models.blip_cap_model is None:
        logger.warning(
            "–ú–æ–¥–µ–ª—å –∏–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä BLIP Caption –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. "
            "–í–æ–∑–≤—Ä–∞—â–∞—é 0.0 –¥–ª—è BLIP Caption + BERTScore."
        )
        return 0.0

    prompt = prompt.strip()
    if not prompt:
        logger.debug("–ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è BLIP Caption + BERTScore –ø—É—Å—Ç. –í–æ–∑–≤—Ä–∞—â–∞—é 0.0.")
        return 0.0

    logger.debug(f"–í—ã—á–∏—Å–ª—è—é BLIP Caption + BERTScore –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞: '{prompt[:50]}...'")

    # --- –ò–ù–ò–¶–ò–ê–õ–ò–ó–ò–†–£–ï–ú –ü–ï–†–ï–ú–ï–ù–ù–£–Æ –î–û –ë–õ–û–ö–ê TRY ---
    generated_caption: str = "" # <-- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
    # ----------------------------------------------

    try:
        # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è
        inputs_for_caption = models.blip_cap_processor(images=img, return_tensors="pt")

        # 2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è pixel_values (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å)
        if "pixel_values" in inputs_for_caption:
             inputs_for_caption["pixel_values"] = inputs_for_caption["pixel_values"].to(dtype=DTYPE)

        # 3. –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω) –∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        model_gpu = _to_gpu(models.blip_cap_model)
        inputs_moved_for_caption = {k: v.to(model_gpu.device) for k, v in inputs_for_caption.items()}

        # 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è (caption)
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º max_new_tokens –≤–º–µ—Å—Ç–æ max_length
        generated_ids = model_gpu.generate(**inputs_moved_for_caption, max_new_tokens=MAX_SIG_TOK)

        # 5. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è
        # --- –ü–†–ò–°–í–ê–ò–í–ê–ï–ú –ó–ù–ê–ß–ï–ù–ò–ï –í–ù–£–¢–†–ò TRY ---
        generated_caption = models.blip_cap_processor.batch_decode(
            generated_ids, skip_special_tokens=True
        )[0].strip()
        # ----------------------------------------

        logger.debug(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ BLIP: '{generated_caption[:100]}...'")

        if not generated_caption:
            logger.warning("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ BLIP –ø—É—Å—Ç–æ. –í–æ–∑–≤—Ä–∞—â–∞—é 0.0.")
            return 0.0

        # 6. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ BERTScore –º–µ–∂–¥—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏ –∏—Å—Ö–æ–¥–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º
        # bert_score.score –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (Precision, Recall, F1)
        P, R, F1 = bert_score_func(
            [generated_caption], # candidates (—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç)
            [prompt],           # references (–∏—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç)
            lang='en',          # –Ø–∑—ã–∫ (–º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º)
            verbose=False,      # –û—Ç–∫–ª—é—á–∞–µ–º –≤—ã–≤–æ–¥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            device=model_gpu.device.type # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, —á—Ç–æ –∏ –º–æ–¥–µ–ª—å
        )

        # 7. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ F1-–±–∞–ª–ª–∞
        bert_score_value = F1.mean().item()

        logger.debug(
            f"BERTScore (P={P.mean().item():.4f}, R={R.mean().item():.4f}, F1={bert_score_value:.4f}) "
            f"–º–µ–∂–¥—É caption –∏ prompt."
        )
        return bert_score_value

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ BLIP Caption + BERTScore: {e}", exc_info=True) # <-- exc_info=True
        return 0.0
    finally:
        # --- –û–¢–õ–ê–î–ö–ê ---
        logger.debug(f"[BLIP_CAP_DEBUG] get_blip_caption_bertscore –≤–µ—Ä–Ω—É–ª: {bert_score_value}")
        # --------------

# --- –ö–æ–Ω–µ—Ü –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏ ---

# --- –ù–û–í–ê–Ø –ú–ï–¢–†–ò–ö–ê: BLIP-2 Caption + BERTScore ---
@torch.inference_mode()
def get_blip2_caption_bertscore(img: Image.Image, prompt: str) -> float:
    # --- –û–¢–õ–ê–î–ö–ê ---
    logger.debug(f"[BLIP2_CAP_DEBUG] –í—ã–∑–æ–≤ get_blip2_caption_bertscore —Å img={img}, prompt='{prompt[:50]}...'")
    # --------------
    
    # ... (–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è) ...
    if not BERT_SCORE_AVAILABLE:
        logger.debug("–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ `bert_score` –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –í–æ–∑–≤—Ä–∞—â–∞—é 0.0.")
        return 0.0

    # --- –ü–†–û–ë–õ–ï–ú–ê 1: –ù–ï–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê ---
    if models.blip2_cap_processor is None or models.blip2_cap_model is None: # <-- –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ None
        logger.warning("–ú–æ–¥–µ–ª—å –∏–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä BLIP-2 Caption –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. "
                       "–í–æ–∑–≤—Ä–∞—â–∞—é 0.0 –¥–ª—è BLIP-2 Caption + BERTScore.")
        return 0.0
    # --- –ö–û–ù–ï–¶ –ü–†–û–ë–õ–ï–ú–´ 1 ---

    prompt = prompt.strip()
    if not prompt: # <-- –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
        logger.debug("–ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è BLIP-2 Caption + BERTScore –ø—É—Å—Ç. –í–æ–∑–≤—Ä–∞—â–∞—é 0.0.")
        return 0.0

    logger.debug(f"–í—ã—á–∏—Å–ª—è—é BLIP-2 Caption + BERTScore –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞: '{prompt[:50]}...'")

    try:
        # --- –ü–†–û–ë–õ–ï–ú–ê 2: –ù–ï–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ü–û–î–ì–û–¢–û–í–ö–ê INPUTS ---
        inputs = models.blip2_cap_processor(images=img, return_tensors="pt") # <-- –°—Ç–∞—Ä–∞—è —Å—Ç—Ä–æ–∫–∞
        # --- –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï 2 ---
        #inputs = models.blip2_cap_processor(images=img, text=prompt, return_tensors="pt") # <-- –î–æ–±–∞–≤–ª–µ–Ω text=prompt
        # ----------------------
        
        # --- 2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è pixel_values (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å) ---
        if "pixel_values" in inputs:
             inputs["pixel_values"] = inputs["pixel_values"].to(dtype=DTYPE) # <-- –ò—Å–ø–æ–ª—å–∑—É–µ–º DTYPE

        # --- 3. –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω) –∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ ---
        model_gpu = _to_gpu(models.blip2_cap_model) # <-- –ü–µ—Ä–µ–º–µ—â–∞–µ–º –º–æ–¥–µ–ª—å
        inputs_moved = {k: v.to(model_gpu.device) for k, v in inputs.items()} # <-- –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ

        # --- 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è (caption) ---
        generated_ids = model_gpu.generate(**inputs_moved, max_new_tokens=MAX_SIG_TOK) # <-- –û–ö, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å text=prompt –ø—Ä–æ–±–ª–µ–º–æ–π
        # ----------------------
        
        # --- 5. –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è ---
        generated_caption = models.blip2_cap_processor.batch_decode(
            generated_ids, skip_special_tokens=True
        )[0].strip() # <-- [0] –∏ strip()
        # ----------------------

        logger.debug(f"–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ BLIP-2: '{generated_caption[:100]}...'")

        # --- –ü–†–û–ë–õ–ï–ú–ê 5: –ù–ï–ü–†–ê–í–ò–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê ---
        if not generated_caption: # <-- –û–ö
            logger.warning("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ BLIP-2 –ø—É—Å—Ç–æ. –í–æ–∑–≤—Ä–∞—â–∞—é 0.0.")
            return 0.0
        # ----------------------

        # --- 6. –í—ã—á–∏—Å–ª–µ–Ω–∏–µ BERTScore –º–µ–∂–¥—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏ –∏—Å—Ö–æ–¥–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º ---
        P, R, F1 = bert_score_func(
            [generated_caption], # candidates (—Å–ø–∏—Å–æ–∫)
            [prompt],           # references (—Å–ø–∏—Å–æ–∫)
            lang='en',
            verbose=False,
            device=model_gpu.device.type
        )
        # ----------------------

        # --- 7. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ä–µ–¥–Ω–µ–≥–æ F1-–±–∞–ª–ª–∞ ---
        bert_score_value = F1.mean().item()

        logger.debug(
            f"BERTScore (P={P.mean().item():.4f}, R={R.mean().item():.4f}, F1={bert_score_value:.4f}) "
            f"–º–µ–∂–¥—É BLIP-2 caption –∏ prompt."
        )
        return bert_score_value

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ BLIP-2 Caption + BERTScore: {e}", exc_info=True) # <-- exc_info=True
        return 0.0
    finally:
        # --- –û–¢–õ–ê–î–ö–ê ---
        logger.debug(f"[BLIP2_CAP_DEBUG] get_blip2_caption_bertscore –≤–µ—Ä–Ω—É–ª: {bert_score_value}")
        # --------------
        if 'model_gpu' in locals(): # <-- –ü—Ä–æ–≤–µ—Ä–∫–∞, –±—ã–ª–∞ –ª–∏ —Å–æ–∑–¥–∞–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
                _release(model_gpu) # <-- –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –º–æ–¥–µ–ª—å
            # ----------------------
# --- –ö–æ–Ω–µ—Ü –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏ ---

# ‚îÄ‚îÄ‚îÄ ImageReward metric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@torch.inference_mode()
def get_imr_score(img: Image.Image, txts: List[str]) -> float:  # noqa: D401
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ä–µ–¥–Ω–∏–π ImageReward‚Äë–±–∞–ª–ª –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤.

    –§—É–Ω–∫—Ü–∏—è –ø–æ–≤—Ç–æ—Ä—è–µ—Ç —Å—Ç–∏–ª—å `get_siglip_score`: –ø—Ä–∏–Ω–∏–º–∞–µ—Ç PIL‚Äë–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏
    —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ (—á–∞–Ω–∫–æ–≤ –ø—Ä–æ–º–ø—Ç–∞ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤).

    –ï—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è ``0.0``. –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è
    –≤—ã–∑–æ–≤–æ–º :func:`load_models`.

    Parameters
    ----------
    img : PIL.Image.Image
        –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
    txts : list[str]
        –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–æ–±—ã—á–Ω–æ –æ–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∞–Ω–∫–æ–≤).

    Returns
    -------
    float
        –°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª ImageReward. –î–∏–∞–ø–∞–∑–æ–Ω ‚âà 0‚Äì10.
    """

    if models.imr_model is None:  # type: ignore[attr-defined]
        raise RuntimeError(
            "models.imr_model == None. load_models() must be called before ranking."
        )

    if not txts:
        logger.debug("–°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è ImageReward –ø—É—Å—Ç. –í–æ–∑–≤—Ä–∞—â–∞—é 0.0.")
        return 0.0

    logger.debug("–í—ã—á–∏—Å–ª—è—é ImageReward –¥–ª—è %d —Ç–µ–∫—Å—Ç–æ–≤.", len(txts))
    with torch.cuda.amp.autocast(enabled=False):     # üîí –≤—ã–∫–ª—é—á–∞–µ–º autocast
        scores = [models.imr_model.score(prompt=t, image=img) for t in txts]  # type: ignore[attr-defined]
    return float(sum(scores) / len(scores))

# --- –ö–æ–Ω–µ—Ü –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏ ---

# ‚îÄ‚îÄ‚îÄ TIFA metric ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    @torch.inference_mode()
    def get_tifa_score(img: Image.Image, prompt: str) -> float:
        if models.tifa_evaluator is None:
            raise RuntimeError("TIFA evaluator not loaded")
        with torch.cuda.amp.autocast(enabled=False):
            return float(models.tifa_evaluator.evaluate(prompt, img))

# --- –ö–æ–Ω–µ—Ü –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏ ---

# --- –®–∞–±–ª–æ–Ω –¥–ª—è –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏ ---
# –ü—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏, —Å–æ–∑–¥–∞–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏:
#
# @torch.inference_mode()
# def get_new_metric_score(img: Image.Image, ...) -> float:
#     """
#     –ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏.
#
#     Args:
#         img (PIL.Image.Image): –í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.
#         ... (–¥—Ä—É–≥–∏–µ –∞—Ä–≥—É–º–µ–Ω—Ç—ã): –ê—Ä–≥—É–º–µ–Ω—Ç—ã, —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –º–µ—Ç—Ä–∏–∫–∏.
#
#     Returns:
#         float: –û—Ü–µ–Ω–∫–∞ –ø–æ –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–µ.
#     """
#     # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–∏–∫–∏ –Ω–æ–≤–æ–π –º–µ—Ç—Ä–∏–∫–∏
#     # –ù–µ –∑–∞–±—É–¥—å—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å _to_gpu/_release, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ
#     # –º–æ–¥–µ–ª–∏, –∫—Ä–æ–º–µ Florence-2.
#     # ...
#     return score
#
# –ó–∞—Ç–µ–º –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–π—Ç–µ –µ—ë –≤ `ranking.py`:
# 1. –î–æ–±–∞–≤—å—Ç–µ –∏–º–ø–æ—Ä—Ç –≤ `ranking.py`.
# 2. –í—ã–∑–æ–≤–∏—Ç–µ –µ—ë –≤ —Ü–∏–∫–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
# 3. –î–æ–±–∞–≤—å—Ç–µ –µ—ë –≤–µ—Å –≤ –∞—Ä–≥—É–º–µ–Ω—Ç—ã CLI –∏ `rank_folder`.
# 4. –í–∫–ª—é—á–∏—Ç–µ –µ—ë –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ `total` –±–∞–ª–ª–∞.