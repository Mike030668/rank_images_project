{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b474b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Путь к проекту добавлен в sys.path\n",
      "[ТЕСТ 1] УСПЕХ: BLIP2_ITM_MODEL_NAME импортировано.\n",
      "[ТЕСТ 1] Значение BLIP2_ITM_MODEL_NAME: 'Salesforce/blip2-itm-vit-g'\n",
      "[ТЕСТ 1] Значение BLIP2_SIZE: 'base'\n",
      "\n",
      "--- Тест 2: Попытка загрузки модели BLIP-2 ITM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаю модель 'Salesforce/blip2-itm-vit-g'...\n",
      "  Процессор BLIP-2 ITM загружен.\n",
      "  Модель BLIP-2 ITM загружена на CPU.\n",
      "  Тест 2: УСПЕХ. Модель загружается корректно.\n",
      "\n",
      "--- Тестирование BLIP2_ITM_MODEL_NAME завершено ---\n"
     ]
    }
   ],
   "source": [
    "# Тест 1: Проверка импорта и значения BLIP2_ITM_MODEL_NAME\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Убедитесь, что путь к проекту корректен\n",
    "project_root = Path('/content/rank_images_project') # ИЛИ ваш локальный путь\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "print(\"Путь к проекту добавлен в sys.path\")\n",
    "\n",
    "# Импортируем config и проверяем\n",
    "try:\n",
    "    from rank_images.config import BLIP2_ITM_MODEL_NAME, BLIP2_SIZE\n",
    "    print(f\"[ТЕСТ 1] УСПЕХ: BLIP2_ITM_MODEL_NAME импортировано.\")\n",
    "    print(f\"[ТЕСТ 1] Значение BLIP2_ITM_MODEL_NAME: '{BLIP2_ITM_MODEL_NAME}'\")\n",
    "    print(f\"[ТЕСТ 1] Значение BLIP2_SIZE: '{BLIP2_SIZE}'\")\n",
    "except ImportError as e:\n",
    "    print(f\"[ТЕСТ 1] ОШИБКА: Не удалось импортировать BLIP2_ITM_MODEL_NAME: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ТЕСТ 1] НЕОЖИДАННАЯ ОШИБКА: {e}\")\n",
    "\n",
    "# Тест 2: Попытка загрузки модели BLIP-2 ITM\n",
    "print(\"\\n--- Тест 2: Попытка загрузки модели BLIP-2 ITM ---\")\n",
    "try:\n",
    "    from transformers import Blip2Processor, Blip2ForImageTextRetrieval\n",
    "    import torch\n",
    "    \n",
    "    print(f\"Загружаю модель '{BLIP2_ITM_MODEL_NAME}'...\")\n",
    "    # Загружаем процессор\n",
    "    blip2_processor_test = Blip2Processor.from_pretrained(BLIP2_ITM_MODEL_NAME)\n",
    "    print(\"  Процессор BLIP-2 ITM загружен.\")\n",
    "    \n",
    "    # Загружаем модель на CPU\n",
    "    blip2_model_test = Blip2ForImageTextRetrieval.from_pretrained(\n",
    "        BLIP2_ITM_MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"cpu\"\n",
    "    ).eval()\n",
    "    print(\"  Модель BLIP-2 ITM загружена на CPU.\")\n",
    "    print(\"  Тест 2: УСПЕХ. Модель загружается корректно.\")\n",
    "    \n",
    "    # Очищаем память теста\n",
    "    del blip2_processor_test, blip2_model_test\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ТЕСТ 2] ОШИБКА: Не удалось загрузить модель '{BLIP2_ITM_MODEL_NAME}': {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n--- Тестирование BLIP2_ITM_MODEL_NAME завершено ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4346a06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Путь к проекту добавлен в sys.path\n",
      "[ТЕСТ 1] УСПЕХ: BLIP2_ITM_MODEL_NAME импортировано.\n",
      "[ТЕСТ 1] Значение BLIP2_ITM_MODEL_NAME: 'Salesforce/blip2-itm-vit-g'\n",
      "[ТЕСТ 1] Значение BLIP2_SIZE: 'base'\n",
      "\n",
      "--- Тест 2: Попытка загрузки модели BLIP-2 ITM ---\n",
      "Загружаю модель 'Salesforce/blip2-itm-vit-g'...\n",
      "  Процессор BLIP-2 ITM загружен.\n",
      "  Модель BLIP-2 ITM загружена на CPU.\n",
      "  Тест 2: УСПЕХ. Модель загружается корректно.\n",
      "\n",
      "--- Тестирование BLIP2_ITM_MODEL_NAME завершено ---\n"
     ]
    }
   ],
   "source": [
    "# Тест 1: Проверка импорта и значения BLIP2_ITM_MODEL_NAME\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Убедитесь, что путь к проекту корректен\n",
    "project_root = Path('/content/rank_images_project') # ИЛИ ваш локальный путь\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "print(\"Путь к проекту добавлен в sys.path\")\n",
    "\n",
    "# Импортируем config и проверяем\n",
    "try:\n",
    "    from rank_images.config import BLIP2_ITM_MODEL_NAME, BLIP2_SIZE\n",
    "    print(f\"[ТЕСТ 1] УСПЕХ: BLIP2_ITM_MODEL_NAME импортировано.\")\n",
    "    print(f\"[ТЕСТ 1] Значение BLIP2_ITM_MODEL_NAME: '{BLIP2_ITM_MODEL_NAME}'\")\n",
    "    print(f\"[ТЕСТ 1] Значение BLIP2_SIZE: '{BLIP2_SIZE}'\")\n",
    "except ImportError as e:\n",
    "    print(f\"[ТЕСТ 1] ОШИБКА: Не удалось импортировать BLIP2_ITM_MODEL_NAME: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"[ТЕСТ 1] НЕОЖИДАННАЯ ОШИБКА: {e}\")\n",
    "\n",
    "# Тест 2: Попытка загрузки модели BLIP-2 ITM\n",
    "print(\"\\n--- Тест 2: Попытка загрузки модели BLIP-2 ITM ---\")\n",
    "try:\n",
    "    from transformers import Blip2Processor, Blip2ForImageTextRetrieval\n",
    "    import torch\n",
    "    \n",
    "    print(f\"Загружаю модель '{BLIP2_ITM_MODEL_NAME}'...\")\n",
    "    # Загружаем процессор\n",
    "    blip2_processor_test = Blip2Processor.from_pretrained(BLIP2_ITM_MODEL_NAME)\n",
    "    print(\"  Процессор BLIP-2 ITM загружен.\")\n",
    "    \n",
    "    # Загружаем модель на CPU\n",
    "    blip2_model_test = Blip2ForImageTextRetrieval.from_pretrained(\n",
    "        BLIP2_ITM_MODEL_NAME,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "        device_map=\"cpu\"\n",
    "    ).eval()\n",
    "    print(\"  Модель BLIP-2 ITM загружена на CPU.\")\n",
    "    print(\"  Тест 2: УСПЕХ. Модель загружается корректно.\")\n",
    "    \n",
    "    # Очищаем память теста\n",
    "    del blip2_processor_test, blip2_model_test\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"[ТЕСТ 2] ОШИБКА: Не удалось загрузить модель '{BLIP2_ITM_MODEL_NAME}': {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n--- Тестирование BLIP2_ITM_MODEL_NAME завершено ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9793abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Путь к проекту добавлен в sys.path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rank_images.models:Начинаю загрузку моделей (на CPU)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] models.blip2_model is: None\n",
      "[TEST] models.blip2_processor is: None\n",
      "BLIP-2 модель или процессор НЕ БЫЛИ загружены. Проверьте логи выше.\n",
      "Начинаю загрузку моделей...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch16/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/openai/clip-vit-base-patch16/57c216476eefef5ab752ec549e440a49ae4ae5f3/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch16/resolve/main/tokenizer_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/openai/clip-vit-base-patch16/57c216476eefef5ab752ec549e440a49ae4ae5f3/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/openai/clip-vit-base-patch16/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/openai/clip-vit-base-patch16/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch16/resolve/main/processor_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch16/resolve/main/chat_template.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch16/resolve/main/chat_template.jinja HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /openai/clip-vit-base-patch16/resolve/main/audio_tokenizer_config.json HTTP/1.1\" 404 0\n",
      "INFO:rank_images.models:Модель CLIP-IQA загружена.\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip2-base-patch16-224/resolve/main/processor_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip2-base-patch16-224/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/google/siglip2-base-patch16-224/75de2d55ec2d0b4efc50b3e9ad70dba96a7b2fa2/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip2-base-patch16-224/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/google/siglip2-base-patch16-224/75de2d55ec2d0b4efc50b3e9ad70dba96a7b2fa2/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip2-base-patch16-224/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/google/siglip2-base-patch16-224/75de2d55ec2d0b4efc50b3e9ad70dba96a7b2fa2/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip2-base-patch16-224/resolve/main/tokenizer_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/google/siglip2-base-patch16-224/75de2d55ec2d0b4efc50b3e9ad70dba96a7b2fa2/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/google/siglip2-base-patch16-224/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/google/siglip2-base-patch16-224/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip2-base-patch16-224/resolve/main/processor_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip2-base-patch16-224/resolve/main/chat_template.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip2-base-patch16-224/resolve/main/chat_template.jinja HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip2-base-patch16-224/resolve/main/audio_tokenizer_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /google/siglip2-base-patch16-224/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/google/siglip2-base-patch16-224/75de2d55ec2d0b4efc50b3e9ad70dba96a7b2fa2/config.json HTTP/1.1\" 200 0\n",
      "INFO:rank_images.models:Модель SigLIP-2 загружена.\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/dinov2-base/resolve/main/processor_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/dinov2-base/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/facebook/dinov2-base/f9e44c814b77203eaa57a6bdbbd535f21ede1415/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/dinov2-base/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/facebook/dinov2-base/f9e44c814b77203eaa57a6bdbbd535f21ede1415/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/dinov2-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/dinov2-base/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/facebook/dinov2-base/f9e44c814b77203eaa57a6bdbbd535f21ede1415/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/dinov2-base/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/dinov2-base/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/facebook/dinov2-base/f9e44c814b77203eaa57a6bdbbd535f21ede1415/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/dinov2-base/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/facebook/dinov2-base/f9e44c814b77203eaa57a6bdbbd535f21ede1415/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /facebook/dinov2-base/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/facebook/dinov2-base/f9e44c814b77203eaa57a6bdbbd535f21ede1415/config.json HTTP/1.1\" 200 0\n",
      "INFO:rank_images.models:Модель DINOv2 загружена.\n",
      "INFO:rank_images.models:Попытка загрузить Florence-2 из локального кэша...\n",
      "/home/mike030668/.venv/rank_images_project/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /microsoft/Florence-2-base/resolve/main/custom_generate/generate.py HTTP/1.1\" 404 0\n",
      "INFO:rank_images.models:Florence-2 успешно загружена из локального кэша.\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /microsoft/Florence-2-base/resolve/main/processor_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /microsoft/Florence-2-base/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /microsoft/Florence-2-base/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /microsoft/Florence-2-base/resolve/main/processing_florence2.py HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/processing_florence2.py HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /microsoft/Florence-2-base/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /microsoft/Florence-2-base/resolve/main/tokenizer_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/microsoft/Florence-2-base/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /microsoft/Florence-2-base/resolve/main/configuration_florence2.py HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/microsoft/Florence-2-base/ceaf371f01ef66192264811b390bccad475a4f02/configuration_florence2.py HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/microsoft/Florence-2-base/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /microsoft/Florence-2-base/resolve/main/processor_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /microsoft/Florence-2-base/resolve/main/chat_template.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /microsoft/Florence-2-base/resolve/main/chat_template.jinja HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /microsoft/Florence-2-base/resolve/main/audio_tokenizer_config.json HTTP/1.1\" 404 0\n",
      "INFO:rank_images.models:Начинаю загрузку модели BLIP-2...\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Salesforce/blip2-itm-vit-g/resolve/main/preprocessor_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/Salesforce/blip2-itm-vit-g/520bf73fd0ef6ccce791cd3f06908040aef2d8cc/preprocessor_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Salesforce/blip2-itm-vit-g/resolve/main/tokenizer_config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/Salesforce/blip2-itm-vit-g/520bf73fd0ef6ccce791cd3f06908040aef2d8cc/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/Salesforce/blip2-itm-vit-g/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/Salesforce/blip2-itm-vit-g/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1\" 404 64\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Salesforce/blip2-itm-vit-g/resolve/main/processor_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Salesforce/blip2-itm-vit-g/resolve/main/chat_template.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Salesforce/blip2-itm-vit-g/resolve/main/chat_template.jinja HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Salesforce/blip2-itm-vit-g/resolve/main/audio_tokenizer_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Salesforce/blip2-itm-vit-g/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/Salesforce/blip2-itm-vit-g/520bf73fd0ef6ccce791cd3f06908040aef2d8cc/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /Salesforce/blip2-itm-vit-g/resolve/main/config.json HTTP/1.1\" 307 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /api/resolve-cache/models/Salesforce/blip2-itm-vit-g/520bf73fd0ef6ccce791cd3f06908040aef2d8cc/config.json HTTP/1.1\" 200 0\n",
      "INFO:rank_images.models:Модель BLIP-2 (ITM) загружена.\n",
      "INFO:rank_images.models:Все модели успешно загружены и готовы к использованию.\n",
      "DEBUG:rank_images.metrics:Вычисляю BLIP-2 ITM скор для 2 промптов.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка моделей завершена.\n",
      "BLIP-2 модель или процессор НЕ БЫЛИ загружены. Проверьте логи выше.\n",
      "\n",
      "--- Тест 1: Простые искусственные данные ---\n",
      "Тестовое изображение: <PIL.Image.Image image mode=RGB size=224x224 at 0x7E7385199C50>\n",
      "Тестовые промпты: ['a red square', 'a blue circle']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in BLIP-2 should be done in processing. Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\n",
      "DEBUG:rank_images.metrics:  Промпт 'a red square...': logits_per_image.shape = torch.Size([1, 2])\n",
      "DEBUG:rank_images.metrics:  Промпт 'a red square...': вероятность соответствия = 0.9224\n",
      "DEBUG:rank_images.metrics:  Промпт 'a blue circle...': logits_per_image.shape = torch.Size([1, 2])\n",
      "DEBUG:rank_images.metrics:  Промпт 'a blue circle...': вероятность соответствия = 0.0106\n",
      "DEBUG:rank_images.metrics:Средняя вероятность соответствия BLIP-2 (ITM) по 2 промптам: 0.4665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLIP-2 Score (искусственные данные): 0.4664764404296875\n",
      "\n",
      "--- Тест 2: Реальное изображение из demo_images ---\n",
      "Файл демонстрационного изображения не найден. Пропуск теста 2.\n",
      "\n",
      "--- Тест 3: Принудительное использование CPU ---\n",
      "Принудительный тест BLIP-2 на CPU...\n",
      "Входные тензоры созданы. Ключи: ['input_ids', 'attention_mask', 'pixel_values']\n",
      "  input_ids: shape=torch.Size([1, 5]), dtype=torch.int64\n",
      "  attention_mask: shape=torch.Size([1, 5]), dtype=torch.int64\n",
      "  pixel_values: shape=torch.Size([1, 3, 224, 224]), dtype=torch.float32\n",
      "Модель и данные перемещены на CPU.\n",
      "Инференс на CPU выполнен успешно.\n",
      "Ошибка в Тесте 3 (CPU): index 1 is out of bounds for dimension 1 with size 1\n",
      "\n",
      "--- Тестирование BLIP-2 завершено ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_28743/3736084789.py\", line 131, in <module>\n",
      "    match_probs = probs[:, 1]\n",
      "                  ~~~~~^^^^^^\n",
      "IndexError: index 1 is out of bounds for dimension 1 with size 1\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForImageTextRetrieval\n",
    "\n",
    "# Ячейка 1: Настройка и импорт\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "# Настройка логирования для отладки\n",
    "logging.basicConfig(level=logging.DEBUG) # Или logging.INFO для менее подробного вывода\n",
    "\n",
    "# Убедитесь, что путь к вашему проекту корректен\n",
    "project_root = Path('/content/rank_images_project') # ИЛИ путь на вашей локальной машине\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "print(\"Путь к проекту добавлен в sys.path\")\n",
    "# В тестовой ячейке, перед проверкой\n",
    "from rank_images import models\n",
    "print(f\"[TEST] models.blip2_model is: {models.blip2_model}\")\n",
    "print(f\"[TEST] models.blip2_processor is: {models.blip2_processor}\")\n",
    "if models.blip2_model is not None and models.blip2_processor is not None:\n",
    "     print(\"BLIP-2 модель и процессор успешно загружены.\")\n",
    "else:\n",
    "     print(\"BLIP-2 модель или процессор НЕ БЫЛИ загружены. Проверьте логи выше.\")\n",
    "\n",
    "# Ячейка 2: Загрузка модели\n",
    "from rank_images.models import load_models, blip2_model, blip2_processor\n",
    "\n",
    "print(\"Начинаю загрузку моделей...\")\n",
    "try:\n",
    "    load_models()\n",
    "    print(\"Загрузка моделей завершена.\")\n",
    "\n",
    "    if blip2_model is not None and blip2_processor is not None:\n",
    "        print(\"BLIP-2 модель и процессор успешно загружены.\")\n",
    "        print(f\"  Модель на устройстве: {blip2_model.device}\")\n",
    "        print(f\"  Тип данных модели: {next(blip2_model.parameters()).dtype}\")\n",
    "    else:\n",
    "        print(\"BLIP-2 модель или процессор НЕ БЫЛИ загружены. Проверьте логи выше.\")\n",
    "        # raise RuntimeError(\"BLIP-2 не загружена\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при загрузке моделей: {e}\")\n",
    "    raise\n",
    "\n",
    "# Ячейка 3: Тестирование функции метрики\n",
    "from rank_images.metrics import get_blip2_match_score\n",
    "import torch\n",
    "\n",
    "# --- 1. Тест с искусственными данными ---\n",
    "print(\"\\n--- Тест 1: Простые искусственные данные ---\")\n",
    "try:\n",
    "    # Создаем очень простое тестовое изображение\n",
    "    test_img = Image.new('RGB', (224, 224), color = 'red')\n",
    "    test_prompts = [\"a red square\", \"a blue circle\"]\n",
    "\n",
    "    print(f\"Тестовое изображение: {test_img}\")\n",
    "    print(f\"Тестовые промпты: {test_prompts}\")\n",
    "\n",
    "    score = get_blip2_match_score(test_img, test_prompts)\n",
    "    print(f\"BLIP-2 Score (искусственные данные): {score}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка в Тесте 1: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# --- 2. Тест с реальным изображением из demo_images ---\n",
    "print(\"\\n--- Тест 2: Реальное изображение из demo_images ---\")\n",
    "try:\n",
    "    # Укажите путь к одному из ваших демонстрационных изображений\n",
    "    demo_img_path = project_root / \"data\" / \"demo_images\" / \"111.png\" # Замените на имя вашего файла\n",
    "\n",
    "    if not demo_img_path.exists():\n",
    "        # Попробуем другое стандартное имя\n",
    "        demo_img_path = project_root / \"data\" / \"demo_images\" / \"image1.jpg\"\n",
    "\n",
    "    if demo_img_path.exists():\n",
    "        real_img = Image.open(demo_img_path).convert('RGB')\n",
    "        # Используем промпт из вашего prompts.json или придумаем подходящий\n",
    "        real_prompts = [\"an image\", \"a picture\"] # Простые, универсальные промпты\n",
    "\n",
    "        print(f\"Реальное изображение: {demo_img_path}\")\n",
    "        print(f\"Реальные промпты: {real_prompts}\")\n",
    "\n",
    "        score_real = get_blip2_match_score(real_img, real_prompts)\n",
    "        print(f\"BLIP-2 Score (реальное изображение): {score_real}\")\n",
    "    else:\n",
    "        print(\"Файл демонстрационного изображения не найден. Пропуск теста 2.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка в Тесте 2: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# --- 3. Тест на CPU (если GPU вызывает проблемы) ---\n",
    "print(\"\\n--- Тест 3: Принудительное использование CPU ---\")\n",
    "try:\n",
    "    # Импортируем напрямую для теста\n",
    "    from rank_images import models\n",
    "    import torch\n",
    "\n",
    "    if models.blip2_model is not None and models.blip2_processor is not None:\n",
    "        print(\"Принудительный тест BLIP-2 на CPU...\")\n",
    "\n",
    "        # Создаем простые данные\n",
    "        dummy_img = Image.new('RGB', (224, 224), color = 'green')\n",
    "        dummy_prompts = [\"a green pixel\"]\n",
    "\n",
    "        # Подготовка данных\n",
    "        inputs = models.blip2_processor(images=dummy_img, text=dummy_prompts, return_tensors=\"pt\", padding=True)\n",
    "        print(f\"Входные тензоры созданы. Ключи: {list(inputs.keys())}\")\n",
    "        for k, v in inputs.items():\n",
    "            print(f\"  {k}: shape={v.shape}, dtype={v.dtype}\")\n",
    "\n",
    "        # Перемещение на CPU\n",
    "        model_cpu = models.blip2_model.to(torch.device(\"cpu\"))\n",
    "        inputs_cpu = {k: v.to(torch.device(\"cpu\")) for k, v in inputs.items()}\n",
    "\n",
    "        print(\"Модель и данные перемещены на CPU.\")\n",
    "\n",
    "        # Инференс\n",
    "        with torch.no_grad(): # Важно для инференса\n",
    "            outputs = model_cpu(**inputs_cpu)\n",
    "\n",
    "        print(\"Инференс на CPU выполнен успешно.\")\n",
    "\n",
    "        # Постобработка\n",
    "        logits_per_text = outputs.logits_per_text\n",
    "        probs = torch.nn.functional.softmax(logits_per_text, dim=-1)\n",
    "        match_probs = probs[:, 1]\n",
    "        avg_prob = match_probs.mean().item()\n",
    "\n",
    "        print(f\"Score на CPU: {avg_prob}\")\n",
    "\n",
    "        # Возврат модели на исходное устройство (если оно было GPU)\n",
    "        # models.blip2_model.to(models.blip2_model.device) # Этот шаг может быть не нужен или вызвать ошибку, если модель была на CPU\n",
    "\n",
    "    else:\n",
    "        print(\"BLIP-2 модель не загружена, тест на CPU пропущен.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка в Тесте 3 (CPU): {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n--- Тестирование BLIP-2 завершено ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fce40008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-25 21:19:16,245 [INFO] rank_images.pipeline_config: Путь к конфигурации пайплайна не указан. Использую стандартную конфигурацию.\n",
      "2025-07-25 21:19:16,253 [INFO] rank_images.cli: Запуск в демонстрационном режиме.\n",
      "2025-07-25 21:19:16,253 [INFO] rank_images.cli: Папка с изображениями: /mnt/c/Users/user/Мой диск (puzitski.mikhail@gmail.com)/СБЕР/TASKS/Control_Generation/rank_images_project/data/demo_images\n",
      "2025-07-25 21:19:16,253 [INFO] rank_images.cli: Файл с промптами: /mnt/c/Users/user/Мой диск (puzitski.mikhail@gmail.com)/СБЕР/TASKS/Control_Generation/rank_images_project/data/demo_images/prompts.json\n",
      "2025-07-25 21:19:16,258 [INFO] rank_images.cli: Загружаю модели (на CPU)...\n",
      "2025-07-25 21:19:16,258 [INFO] rank_images.models: Начинаю загрузку моделей (на CPU)...\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "2025-07-25 21:19:19,645 [INFO] rank_images.models: Модель CLIP-IQA загружена.\n",
      "2025-07-25 21:19:24,647 [INFO] rank_images.models: Модель SigLIP-2 загружена.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "2025-07-25 21:19:26,435 [INFO] rank_images.models: Модель DINOv2 загружена.\n",
      "2025-07-25 21:19:26,435 [INFO] rank_images.models: Попытка загрузить Florence-2 из локального кэша...\n",
      "/home/mike030668/.venv/rank_images_project/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "2025-07-25 21:19:26,914 [INFO] accelerate.utils.modeling: We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2025-07-25 21:20:00,767 [INFO] rank_images.models: Florence-2 успешно загружена из локального кэша.\n",
      "2025-07-25 21:20:03,266 [INFO] rank_images.models: Начинаю загрузку модели BLIP-2...\n",
      "2025-07-25 21:20:06,715 [INFO] rank_images.models: Модель BLIP-2 (ITM) загружена.\n",
      "2025-07-25 21:20:06,715 [INFO] rank_images.models: Начинаю загрузку модели BLIP Caption...\n",
      "model.safetensors:   8%|██                         | 83.9M/1.07G [00:00<?, ?B/s]2025-07-25 21:20:10,829 [INFO] rank_images.models: Модель BLIP Caption загружена.\n",
      "2025-07-25 21:20:10,830 [INFO] rank_images.models: Все модели успешно загружены и готовы к использованию.\n",
      "2025-07-25 21:20:10,830 [INFO] rank_images.cli: Все модели успешно загружены.\n",
      "2025-07-25 21:20:10,830 [INFO] rank_images.cli: Начинаю процесс ранжирования...\n",
      "2025-07-25 21:20:10,830 [INFO] rank_images.ranking: Начинаю ранжирование изображений в папке: /mnt/c/Users/user/Мой диск (puzitski.mikhail@gmail.com)/СБЕР/TASKS/Control_Generation/rank_images_project/data/demo_images\n",
      "2025-07-25 21:20:10,830 [INFO] rank_images.ranking: Включённые метрики: ['sig', 'flor', 'iqa', 'dino', 'blip2', 'blip_cap']\n",
      "2025-07-25 21:20:10,850 [INFO] rank_images.data_processing: Загружаю промпты из JSON файла: /mnt/c/Users/user/Мой диск (puzitski.mikhail@gmail.com)/СБЕР/TASKS/Control_Generation/rank_images_project/data/demo_images/prompts.json\n",
      "2025-07-25 21:20:10,869 [INFO] rank_images.data_processing: Промпты предоставлены в виде словаря.\n",
      "2025-07-25 21:20:10,871 [INFO] rank_images.ranking: Начинаю вычисление метрик для изображений...\n",
      "\n",
      "model.safetensors:  10%|█▉                  | 105M/1.07G [00:03<02:21, 6.83MB/s]\u001b[AExpanding inputs for image tokens in BLIP-2 should be done in processing. Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. Using processors without these attributes in the config is deprecated and will throw an error in v4.50.\n",
      "model.safetensors:  12%|██▎                 | 126M/1.07G [00:06<02:15, 7.00MB/s]\n",
      "\n",
      "model.safetensors:  13%|██▌                 | 136M/1.07G [00:07<02:13, 7.03MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   0%|                    | 1.57M/1.42G [00:02<30:18, 781kB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  14%|██▋                 | 147M/1.07G [00:12<03:49, 4.04MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  17%|███▎                | 178M/1.07G [00:40<10:17, 1.45MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:   6%|█                  | 81.3M/1.42G [00:35<10:01, 2.23MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  10%|██                  | 148M/1.42G [00:38<04:43, 4.49MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  15%|███                 | 215M/1.42G [00:39<02:36, 7.70MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  20%|███▉                | 282M/1.42G [00:39<01:32, 12.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  34%|██████▊             | 483M/1.42G [00:39<00:29, 31.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  48%|█████████▋          | 684M/1.42G [00:39<00:12, 58.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  58%|███████████▌        | 818M/1.42G [00:39<00:07, 83.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  18%|███▌                | 189M/1.07G [00:47<09:54, 1.49MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  81%|████████████████▏   | 1.15G/1.42G [00:40<00:01, 165MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors:  91%|██████████████████  | 1.29G/1.42G [00:42<00:01, 127MB/s]\u001b[A\u001b[A\n",
      "\n",
      "model.safetensors: 100%|███████████████████| 1.42G/1.42G [00:42<00:00, 33.7MB/s]\u001b[A\u001b[A\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "model.safetensors:  19%|███▋                | 199M/1.07G [00:49<07:45, 1.88MB/s]\n",
      "model.safetensors:  21%|████▎               | 231M/1.07G [00:54<03:56, 3.57MB/s]\u001b[ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "model.safetensors:  24%|████▉               | 262M/1.07G [00:58<02:33, 5.29MB/s]\u001b[ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "model.safetensors:  27%|█████▍              | 294M/1.07G [01:03<02:02, 6.34MB/s]\u001b[ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "model.safetensors:  30%|██████              | 325M/1.07G [01:07<01:50, 6.75MB/s]\u001b[ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "model.safetensors:  32%|██████▍             | 346M/1.07G [01:12<02:19, 5.21MB/s]\u001b[ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "model.safetensors:  35%|███████             | 377M/1.07G [01:17<02:00, 5.80MB/s]\u001b[ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "model.safetensors:  38%|███████▌            | 409M/1.07G [01:22<01:41, 6.58MB/s]\u001b[ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "model.safetensors:  41%|████████▏           | 440M/1.07G [01:26<01:34, 6.72MB/s]\u001b[ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "model.safetensors:  44%|████████▊           | 472M/1.07G [01:31<01:29, 6.73MB/s]\u001b[ASome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Обработка изображений: 100%|████████████████████| 10/10 [01:31<00:00,  9.12s/it]\u001b[A\n",
      "2025-07-25 21:21:42,080 [INFO] rank_images.ranking: Успешно обработано 10 изображений.\n",
      "2025-07-25 21:21:42,080 [INFO] rank_images.ranking: Нормализую метрики и вычисляю итоговый балл...\n",
      "2025-07-25 21:21:42,148 [INFO] rank_images.ranking: Результаты ранжирования сохранены в: /mnt/c/Users/user/Мой диск (puzitski.mikhail@gmail.com)/СБЕР/TASKS/Control_Generation/rank_images_project/data/demo_images/ranking.csv\n",
      "2025-07-25 21:21:42,148 [INFO] rank_images.cli: Процесс ранжирования завершён успешно.\n",
      "\n",
      "--- Результаты ранжирования (первые 5 строк) ---\n",
      "      image       sig      flor  ...  blip2_norm  blip_cap_norm     total\n",
      "0   222.png  0.201782 -0.370370  ...    1.002061       0.795361  0.609162\n",
      "1   666.png  0.209412 -0.382716  ...    1.014775      -0.924521  0.228866\n",
      "2   999.png  0.189972 -0.370370  ...    1.143937       0.342924  0.207013\n",
      "3   777.png  0.207397 -0.382716  ...   -1.085315      -0.295471  0.154222\n",
      "4  1010.png  0.196503 -0.370370  ...   -1.003240       0.410766  0.107697\n",
      "\n",
      "[5 rows x 14 columns]\n",
      "-----------------------------------------------\n",
      "Полный результат сохранён в: /mnt/c/Users/user/Мой диск (puzitski.mikhail@gmail.com)/СБЕР/TASKS/Control_Generation/rank_images_project/data/demo_images/ranking.csv\n",
      "model.safetensors:  92%|██████████████████▍ | 990M/1.07G [02:46<00:15, 5.45MB/s]\n"
     ]
    }
   ],
   "source": [
    "!rank-images --demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "134a7b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'projects/Control_generation/rank_images_project'\n",
      "/mnt/c/Users/user/Мой диск (puzitski.mikhail@gmail.com)/СБЕР/TASKS/Control_Generation/rank_images_project/notebooks\n",
      "2025-07-25 22:32:45,055 [WARNING] rank_images.pipeline_config: Файл конфигурации пайплайна 'configs/caption_focus_pipeline.json' не найден. Использую стандартную конфигурацию.\n",
      "2025-07-25 22:32:45,056 [ERROR] rank_images.cli: Указанная директория изображений не существует или не является папкой: data/demo_images\n"
     ]
    }
   ],
   "source": [
    "%cd projects/Control_generation/rank_images_project\n",
    "!rank-images data/demo_images \\\n",
    "  --prompts data/demo_images/prompts.json \\\n",
    "  --pipeline-config configs/caption_focus_pipeline.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1f79a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd295ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PIL'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Настройка логирования для отладки\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'PIL'"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForImageTextRetrieval\n",
    "\n",
    "# Ячейка 1: Настройка и импорт\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import logging\n",
    "\n",
    "# Настройка логирования для отладки\n",
    "logging.basicConfig(level=logging.DEBUG) # Или logging.INFO для менее подробного вывода\n",
    "\n",
    "# Убедитесь, что путь к вашему проекту корректен\n",
    "project_root = Path('/content/rank_images_project') # ИЛИ путь на вашей локальной машине\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "print(\"Путь к проекту добавлен в sys.path\")\n",
    "\n",
    "# 1. --- Настройка окружения ---\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# --- Настройка логирования ---\n",
    "# Включаем DEBUG логи для нашего проекта\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, # Или logging.INFO для менее подробного вывода\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)] # Вывод в ячейку\n",
    ")\n",
    "print(\"Логирование настроено.\")\n",
    "\n",
    "# --- Добавление пути к проекту ---\n",
    "project_root = Path('/content/rank_images_project') # <-- Укажите правильный путь\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "print(f\"Путь к проекту '{project_root}/src' добавлен в sys.path\")\n",
    "# -------------------------------\n",
    "\n",
    "# 2. --- Импорт необходимых модулей ---\n",
    "from rank_images.pipeline_config import load_pipeline_config, get_enabled_metrics\n",
    "from rank_images.models import load_models, METRIC_TO_MODELS\n",
    "import gc\n",
    "# -----------------------------------\n",
    "\n",
    "# 3. --- Проверка загрузки конфига ---\n",
    "print(\"\\n--- 1. Проверка загрузки конфига ---\")\n",
    "config_path = project_root / \"configs\" / \"caption_focus_pipeline.json\"\n",
    "print(f\"Путь к конфигурационному файлу: {config_path}\")\n",
    "\n",
    "# Проверим, существует ли файл\n",
    "if not config_path.exists():\n",
    "    print(f\"  [ОШИБКА] Файл конфига не найден: {config_path}\")\n",
    "else:\n",
    "    print(f\"  [OK] Файл конфига найден.\")\n",
    "    \n",
    "    # Попробуем загрузить конфиг\n",
    "    try:\n",
    "        pipeline_config = load_pipeline_config(str(config_path))\n",
    "        print(f\"  [OK] Конфиг успешно загружен.\")\n",
    "        print(f\"  Содержимое pipeline_config (первый уровень): {list(pipeline_config.keys())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [ОШИБКА] Не удалось загрузить конфиг: {e}\")\n",
    "        pipeline_config = None\n",
    "\n",
    "# 4. --- Проверка извлечения enabled_metrics ---\n",
    "print(\"\\n--- 2. Проверка извлечения enabled_metrics ---\")\n",
    "if pipeline_config:\n",
    "    try:\n",
    "        enabled_metrics_list = get_enabled_metrics(pipeline_config)\n",
    "        print(f\"  [OK] Список включённых метрик извлечён.\")\n",
    "        print(f\"  enabled_metrics_list: {enabled_metrics_list}\")\n",
    "        \n",
    "        expected_metrics = [\"blip_cap\", \"blip2_cap\", \"iqa\"]\n",
    "        if enabled_metrics_list == expected_metrics:\n",
    "            print(f\"  [OK] Список метрик СООТВЕТСТВУЕТ ожидаемому: {expected_metrics}\")\n",
    "        else:\n",
    "            print(f\"  [ПРЕДУПРЕЖДЕНИЕ] Список метрик НЕ СООТВЕТСТВУЕТ ожидаемому.\")\n",
    "            print(f\"    Ожидаемый:  {expected_metrics}\")\n",
    "            print(f\"    Полученный: {enabled_metrics_list}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [ОШИБКА] Не удалось извлечь enabled_metrics_list: {e}\")\n",
    "        enabled_metrics_list = None\n",
    "else:\n",
    "    print(\"  [ПРОПУЩЕНО] Конфиг не был загружен.\")\n",
    "    enabled_metrics_list = None\n",
    "\n",
    "# 5. --- Подготовка к проверке загрузки моделей ---\n",
    "print(\"\\n--- 3. Подготовка к проверке загрузки моделей ---\")\n",
    "# Сбросим состояние моделей (на всякий случай)\n",
    "# Это имитирует \"чистый\" запуск\n",
    "import rank_images.models as models_module\n",
    "# Сбрасываем глобальные переменные моделей\n",
    "for attr_name in list(dir(models_module)):\n",
    "    if not attr_name.startswith('__') and attr_name not in ['logger']:\n",
    "        if hasattr(models_module, attr_name) and getattr(models_module, attr_name) is not None:\n",
    "            # Проверим, является ли атрибут моделью/процессором (примерная проверка)\n",
    "            attr_value = getattr(models_module, attr_name)\n",
    "            if hasattr(attr_value, 'parameters') or hasattr(attr_value, 'model') or 'processor' in attr_name.lower():\n",
    "                 print(f\"  Сбрасываю атрибут модели: {attr_name}\")\n",
    "                 setattr(models_module, attr_name, None)\n",
    "\n",
    "# Принудительно запускаем сборщик мусора\n",
    "gc.collect()\n",
    "print(\"  Глобальные переменные моделей сброшены (где возможно).\")\n",
    "print(\"  Выполнен сбор мусора.\")\n",
    "\n",
    "# 6. --- Проверка загрузки моделей ---\n",
    "print(\"\\n--- 4. Проверка загрузки моделей ---\")\n",
    "if enabled_metrics_list is not None:\n",
    "    print(f\"  Начинаю загрузку моделей ТОЛЬКО для: {enabled_metrics_list}\")\n",
    "    try:\n",
    "        # Вызываем load_models с нашим списком\n",
    "        load_models(enabled_metrics_list)\n",
    "        print(f\"  [OK] Вызов load_models({enabled_metrics_list}) завершён.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [ОШИБКА] При вызове load_models произошла ошибка: {e}\")\n",
    "else:\n",
    "    print(\"  [ПРОПУЩЕНО] Список метрик не доступен.\")\n",
    "\n",
    "# 7. --- Проверка, какие модели были загружены ---\n",
    "print(\"\\n--- 5. Проверка загруженных моделей ---\")\n",
    "# Определим, какие модели ДОЛЖНЫ были загрузиться\n",
    "expected_loaded_models = set()\n",
    "if enabled_metrics_list:\n",
    "    for metric in enabled_metrics_list:\n",
    "        expected_loaded_models.update(METRIC_TO_MODELS.get(metric, []))\n",
    "    print(f\"  Ожидаемые к загрузке модели: {sorted(expected_loaded_models)}\")\n",
    "else:\n",
    "    print(\"  [ПРОПУЩЕНО] Список метрик не доступен.\")\n",
    "\n",
    "# Проверим, какие модели фактически НЕ равны None\n",
    "actually_loaded_models = set()\n",
    "if pipeline_config: # Проверяем только если конфиг был корректен\n",
    "    for model_attr_name in dir(models_module):\n",
    "        if not model_attr_name.startswith('_'):\n",
    "            model_attr_value = getattr(models_module, model_attr_name)\n",
    "            # Проверяем, что атрибут не None и похож на модель/процессор\n",
    "            if model_attr_value is not None and (\n",
    "                hasattr(model_attr_value, 'parameters') or \n",
    "                hasattr(model_attr_value, 'model') or \n",
    "                'processor' in model_attr_name.lower()\n",
    "            ):\n",
    "                actually_loaded_models.add(model_attr_name)\n",
    "\n",
    "print(f\"  Фактически загруженные модели: {sorted(actually_loaded_models)}\")\n",
    "\n",
    "# Сравним ожидаемое и фактическое\n",
    "if expected_loaded_models and actually_loaded_models:\n",
    "    if expected_loaded_models == actually_loaded_models:\n",
    "        print(f\"  [OK] Фактически загруженные модели СООТВЕТСТВУЮТ ожидаемым.\")\n",
    "    else:\n",
    "        print(f\"  [ПРЕДУПРЕЖДЕНИЕ] Несоответствие в загруженных моделях.\")\n",
    "        print(f\"    Ожидаемые, но не загруженные: {sorted(expected_loaded_models - actually_loaded_models)}\")\n",
    "        print(f\"    Загруженные, но не ожидаемые: {sorted(actually_loaded_models - expected_loaded_models)}\")\n",
    "elif not expected_loaded_models:\n",
    "     print(\"  [ПРОПУЩЕНО] Нет ожидаемых моделей для проверки.\")\n",
    "else: # expected_loaded_models есть, но actually_loaded_models пуст\n",
    "     print(f\"  [ПРЕДУПРЕЖДЕНИЕ] Не было загружено ни одной модели, хотя ожидались: {sorted(expected_loaded_models)}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Проверка завершена ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ranger_generation_fl)",
   "language": "python",
   "name": "ranger_generation_fl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
